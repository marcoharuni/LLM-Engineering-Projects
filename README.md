<div align="center">

<h1>
LLM Engineering Projects
</h1>

<p>
<a href="https://x.com/marcoharuni">
  <img src="https://img.shields.io/badge/Follow%20on%20ğ•-@marcoharuni-000000?style=for-the-badge&logo=x&logoColor=white"/>
</a>
&nbsp;&nbsp;
<a href="https://youtube.com/@marcoharuni">
  <img src="https://img.shields.io/badge/Subscribe-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white"/>
</a>
&nbsp;&nbsp;
<a href="https://github.com/marcoharuni/LLM-Engineering-Projects">
  <img src="https://img.shields.io/badge/Star%20on-GitHub-181717?style=for-the-badge&logo=github&logoColor=white"/>
</a>
&nbsp;&nbsp;
<a href="https://buymeacoffee.com/marcoharuni">
  <img src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-FFDD00?style=for-the-badge&logo=buymeacoffee&logoColor=black"/>
</a>
</p>

<br>

<img src="assets/banner.png" width="1000"/>

<br><br>

<p>
<b>Build modern Large Language Models from mathematics to trillion-parameter systems.</b>
</p>

<p>
Code, maths, and explanations for 20+ projects covering the mechanics and reasoning behind<br>
modern LLMs like <b>DeepSeek</b>, <b>LLaMA</b>, <b>Qwen</b>, <b>Mistral</b>, and more.
</p>

<p>
You will be able to build your own state-of-the-art large language model<br>
and understand the reason behind every architectural decision.
</p>

</div>

---

## âœ¨ Features

- ğŸ”¥ **20+ hands-on projects** â€” from tokenizers to trillion-parameter training
- ğŸ“ **Mathematics explained** â€” clear derivations and intuitions
- ğŸ’» **Code from scratch** â€” PyTorch implementations, no black-box libraries
- ğŸ¥ **Video tutorials** â€” for each project
- ğŸ“ **Blog posts** â€” with deep explanations
- ğŸš€ **Google Colab notebooks** â€” ready to run
- ğŸ§ª **Research-grade implementations** â€” based on latest papers

---

## ğŸ“‘ Table of Contents

- [What You Will Learn](#-what-you-will-learn)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [List of Projects](#-list-of-projects)
- [The Researcher's Mindset](#-the-researchers-mindset)
- [Citation](#-citation)
- [Contributing](#-contributing)
- [Support This Project](#-support-this-project)
- [License](#-license)
- [Acknowledgements](#-acknowledgements)

---

## ğŸ¯ What You Will Learn

By working through these projects, you will:

- **Understand transformers deeply** â€” The mathematics and engineering trade-offs
- **Build from scratch** â€” tokenizers, attention mechanisms, optimizers, and full training pipelines
- **Train models at scale** â€” from 1M to 1B+ parameters with distributed systems
- **Optimize inference** â€” KV cache, speculative decoding, quantization
- **Align models** â€” instruction tuning, RLHF, DPO, and test-time compute and more more

---

## ğŸ“‹ Prerequisites

Before starting these projects, make sure you have foundational knowledge in these areas:

<table>
<tr>
<th align="center">S/N</th>
<th align="left">Topic</th>
<th align="center">Resource</th>
</tr>
<tr>
<td align="center">1</td>
<td><b>Python, Mojo, Rust, C/C++</b></td>
<td align="center"><a href="">ğŸ“ Link</a></td>
</tr>
<tr>
<td align="center">2</td>
<td><b>High School Mathematics</b> (Algebra, Probability, Calculus, Geometry)</td>
<td align="center"><a href="">ğŸ“ Link</a></td>
</tr>
<tr>
<td align="center">3</td>
<td><b>Neural Networks & Deep Learning Fundamentals</b></td>
<td align="center"><a href="">ğŸ“ Link</a></td>
</tr>
<tr>
<td align="center">4</td>
<td><b>PyTorch</b></td>
<td align="center"><a href="">ğŸ“ Link</a></td>
</tr>
<tr>
<td align="center">5</td>
<td><b>GPU Programming</b> (CUDA basics)</td>
<td align="center"><a href="">ğŸ“ Link</a></td>
</tr>
<tr>
<td align="center">6</td>
<td><b>Pandas, NumPy, Matplotlib</b></td>
<td align="center"><a href="">ğŸ“ Link</a></td>
</tr>
<tr>
<td align="center">7</td>
<td><b>CS Fundamentals</b> (Data Structures & Algorithms)</td>
<td align="center"><a href="">ğŸ“ Link</a></td>
</tr>
<tr>
<td align="center">8</td>
<td><b>Computer Architecture</b> (GPU, CPU, Memory Hierarchy)</td>
<td align="center"><a href="">ğŸ“ Link</a></td>
</tr>
</table>

---

## âš™ï¸ Installation

**Option 1: Google Colab (Recommended)**

Click the Colab badge on any project â€” no setup required.

**Option 2: Local Setup**

```bash
# Clone the repository
git clone https://github.com/marcoharuni/LLM-Engineering-Projects.git
cd LLM-Engineering-Projects

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

**Option 3: Cloud GPU Providers**

See the **[Setup Guide](docs/SETUP.md)** for instructions on Lambda Labs, RunPod, and AWS.

---

## ğŸ“š List of Projects

I advise running all projects through **Google Colab** for the easiest setup. If you have your own GPU locally or through a cloud provider, see the **[Setup Guide](docs/SETUP.md)**.

<table>
<tr>
<th align="center">S/N</th>
<th align="left">Project</th>
<th align="center">Video</th>
<th align="center">Blog</th>
<th align="center">Notebook</th>
</tr>
<tr>
<td align="center"><b>01</b></td>
<td>Tokenization & Embeddings</td>
<td align="center"><a href="">â–¶ï¸</a></td>
<td align="center"><a href="">ğŸ“</a></td>
<td align="center"><a href=""><img src="https://colab.research.google.com/assets/colab-badge.svg" height="20"/></a></td>
</tr>
<tr>
<td align="center"><b>02</b></td>
<td>Positional Embeddings</td>
<td align="center"><a href="">â–¶ï¸</a></td>
<td align="center"><a href="">ğŸ“</a></td>
<td align="center"><a href=""><img src="https://colab.research.google.com/assets/colab-badge.svg" height="20"/></a></td>
</tr>
</table>

<!--
TO ADD MORE PROJECTS:
Copy the template below and paste it ABOVE this comment (before </table>)

<tr>
<td align="center"><b>03</b></td>
<td>Project Name Here</td>
<td align="center"><a href="">â–¶ï¸</a></td>
<td align="center"><a href="">ğŸ“</a></td>
<td align="center"><a href=""><img src="https://colab.research.google.com/assets/colab-badge.svg" height="20"/></a></td>
</tr>

Then:
1. Change the number (03, 04, 05...)
2. Change "Project Name Here" to your project title
3. Add your video URL inside href=""
4. Add your blog URL inside href=""
5. Add your Colab URL inside href=""
-->

---

## ğŸ§  The Researcher's Mindset

| Principle | Description |
|-----------|-------------|
| **Ablate Everything** | Don't trust paper claims. Break the code. Swap RoPE for ALiBi mid-training. Does it die? |
| **Visualize Religiously** | Log attention maps to W&B every 100 steps. Plot activation histograms by layer. |
| **Data > Architecture > Hyperparams** | Spend 50% of time on data quality, 30% on architecture, 20% on hyperparameters. |
| **Scale Extrapolation** | Train 1M â†’ 10M â†’ 100M â†’ 1B. Fit scaling laws. Predict 10B performance before training. |
| **Emergency Brakes** | NaN detection. Gradient norm tracking. Kill job if loss > 100 or NaN. |

---

## ğŸ“– Citation

If you use this curriculum in your research or learning journey:

```bibtex
@misc{haruni2026llmengineering,
  author       = {Marco Haruni},
  title        = {LLM Engineering Projects: From Zero to Trillion-Parameter Training},
  year         = {2026},
  publisher    = {GitHub},
  howpublished = {\url{https://github.com/marcoharuni/LLM-Engineering-Projects}}
}
```

---

## ğŸ¤ Contributing

Contributions are welcome and encouraged!

**You can help by:**

- Adding new projects or notebooks
- Improving explanations and math derivations
- Fixing bugs and typos
- Optimizing code and performance
- Writing tutorials or visualizations
- Translating content to other languages

**How to contribute:**

1. Fork the repository
2. Create a new branch (`git checkout -b feature/my-feature`)
3. Make your changes
4. Commit with clear messages (`git commit -m "Add: new attention visualization"`)
5. Push to your branch (`git push origin feature/my-feature`)
6. Open a Pull Request with a short explanation

**Guidelines:**

- Follow clean code standards
- Add clear documentation and comments
- Include examples where helpful
- Test your code before submitting

---

## â˜• Support This Project

If this project helps you learn, saves you time, or supports your research â€” consider buying me a coffee.

Your support helps cover GPU compute costs and keeps this project free and open for everyone.

<p align="center">
<a href="https://buymeacoffee.com/marcoharuni">
  <img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" height="60"/>
</a>
</p>

**Other ways to support:**

- â­ Star this repository
- ğŸ“¢ Share on social media
- ğŸ› Report bugs and issues
- ğŸ’¡ Suggest new projects or improvements

---

## ğŸ“„ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

You are free to use, modify, and distribute this code for personal or commercial purposes.

---

## ğŸ™ Acknowledgements

This curriculum draws inspiration and knowledge from:

**Tutorials & Courses**
- Andrej Karpathy's neural network series
- fast.ai courses
- Stanford CS224N

**Papers & Technical Reports**
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€” Vaswani et al.
- [LLaMA](https://arxiv.org/abs/2302.13971) â€” Meta AI
- [DeepSeek-V3](https://arxiv.org/abs/2412.19437) â€” DeepSeek AI
- [Flash Attention](https://arxiv.org/abs/2205.14135) â€” Dao et al.
- [RoFormer (RoPE)](https://arxiv.org/abs/2104.09864) â€” Su et al.
- [GQA](https://arxiv.org/abs/2305.13245) â€” Ainslie et al.
- [Chinchilla](https://arxiv.org/abs/2203.15556) â€” Hoffmann et al.

**Open Source Projects**
- PyTorch
- Hugging Face Transformers
- vLLM
- lm-eval-harness

And countless late-night debugging sessions and more more ...

---

<div align="center">

<b>Lock in. Build. Break. Plot. Repeat.</b>

<br><br>

**Good luck on your journey.**

<br><br>

<a href="https://x.com/marcoharuni">
  <img src="https://img.shields.io/badge/ğ•-@marcoharuni-000000?style=flat-square&logo=x&logoColor=white"/>
</a>
&nbsp;
<a href="https://youtube.com/@marcoharuni">
  <img src="https://img.shields.io/badge/YouTube-FF0000?style=flat-square&logo=youtube&logoColor=white"/>
</a>
&nbsp;
<a href="https://buymeacoffee.com/marcoharuni">
  <img src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-FFDD00?style=flat-square&logo=buymeacoffee&logoColor=black"/>
</a>

</div>
